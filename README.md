
# ğŸ§  Credit Risk Scoring with Explainable AI (XAI)

A machine learning pipeline to assess customer creditworthiness using real-world German credit data. This project simulates a banking-grade credit scoring model with explainable outputs using SHAP, Weight of Evidence (WoE), Streamlit UI, and Dockerized deployment.

---

## ğŸ“Œ Problem Statement

Financial institutions need to assess whether a customer is likely to default on a loan. This project aims to:
- Build a credit risk prediction model using customer financial and demographic data.
- Apply interpretable ML techniques to help risk analysts and compliance teams understand model decisions.

---

## âš™ï¸ Tech Stack

| Component         | Tool/Library                |
|------------------|-----------------------------|
| Language         | Python                      |
| Data Handling    | Pandas, Numpy               |
| Modeling         | Scikit-learn, GridSearchCV  |
| Feature Encoding | WoE Binning, IV             |
| Explainability   | SHAP                        |
| Visualization    | Matplotlib, Seaborn         |
| Dashboard        | Streamlit                   |
| Deployment       | Docker                      |
| Version Control  | Git, GitHub                 |

---

## ğŸš€ How to Run

### ğŸ’» Local (Python)

```bash
# Clone the repository
git clone https://github.com/YOUR_USERNAME/credit-risk-xai.git
cd credit-risk-xai

# (Optional) Create virtual environment
python -m venv .venv
source .venv/Scripts/activate  # Or . .venv/bin/activate on Mac/Linux

# Install dependencies
pip install -r requirements.txt

# Run training and tuning
python src/train.py
python src/tune_model.py

# Run Streamlit UI
streamlit run app/credit_scoring_app.py
```

---

### ğŸ³ Run with Docker

```bash
# From root folder
docker build -t credit-risk-app .
docker run -p 8501:8501 credit-risk-app
```

Visit ğŸ‘‰ http://localhost:8501 to access the app.

---

## ğŸ“ Project Structure

```
credit-risk-xai/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”œâ”€â”€ notebooks/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_prep.py
â”‚   â”œâ”€â”€ train.py
â”‚   â””â”€â”€ tune_model.py
â”œâ”€â”€ models/
â”œâ”€â”€ app/
â”‚   â””â”€â”€ credit_scoring_app.py
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ daily_notes/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ§ª Model Evaluation

- âœ… Logistic Regression (Baseline + Tuned)
- âœ… Decision Tree (Baseline + Tuned)
- âœ… SHAP Waterfall and Beeswarm explainability
- âœ… ROC Curve & Confusion Matrix
- âœ… Streamlit interactive predictions

Evaluation plots saved in `/models/`:
- `confusion_matrix_logreg.png`
- `roc_curve_logreg.png`
- `shap_beeswarm_logreg.png`
- `shap_waterfall_logreg_sample0.png`
- `shap_beeswarm_tree.png`
- `shap_waterfall_tree_sample0.png`

---

## ğŸ—“ï¸ Project Timeline

| Day         | Highlights |
|-------------|------------|
| Day 01 (9th June)   | Folder setup, dataset loaded, EDA, risk labeling |
| Day 02 (10th June)  | WoE + IV calculated and visualized |
| Day 03 (12th June)  | Baseline models trained + saved |
| Day 04 (13th June)  | Hyperparameter tuning, AUC, ROC, CM evaluation |
| Day 05 (13th June)  | Tuned Decision Tree, model comparison added |
| Day 06 (15th June)  | SHAP Explainability implemented |
| Day 07 (18th June)  | Streamlit UI built + Dockerized for deployment |

---

## ğŸ‘¥ Author

**Yasaswin Palukuri**  
ğŸ“ Data Science & AI Enthusiast  
ğŸ“« [LinkedIn](https://www.linkedin.com/in/yasaswin-palukuri)

---

## ğŸ“Œ License

MIT License. Free to use, modify, and deploy.
